{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習(Deep Neural Networks)\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kyorin-phys/MLIntro2025/blob/main/4/4.ipynb)\n",
    "\n",
    "* 入力データ\n",
    "* 出力データ（正解）\n",
    "\n",
    "入力データと出力データの組を**学習**する\n",
    "\n",
    "入力データから出力データを再現できるような**関数**を作ることができるものを**機械学習**という。\n",
    "\n",
    "関数はパラメータを持つ。線形回帰なら、パラメータ（傾きと切片）を調節して、予測データと正解データの「誤差」が\n",
    "最小となるようなパラメータを求める。(「誤差」の評価はスケール変数とカテゴリ変数で異なる。)\n",
    "\n",
    "深層ニューラルネットワークは隠れ層を入れることでパラメータの数を増やして、近似的な**関数**を作る仕組みである。\n",
    "\n",
    "生物のニューロンにおいて、刺激の総和が閾値を超えると発火するという概念をまねたものを人工ニューロン（パーセプトロン）という。\n",
    "\n",
    "単層パーセプトロン(SLP)：入力と出力しかなく、入力の重み付き和が閾値をこえるかどうかが出力である。線形分類（直線や平面でデータが完全に分けられる）しかできない。\n",
    "$$\n",
    "f(x_1, x_2) = w_1 x_1 + w_2 x_2 + b > 0 \n",
    "$$\n",
    "<img src='SLP.png' width='10%'>\n",
    "\n",
    "\n",
    "多層パーセプトロン(MLP)：隠れ層（中間層）を入れたもの。層を増やすことでパラメータを増やすことと、\n",
    "非線形な活性化関数（例：シグモイド関数、階段関数、ReLU、softmax）を使うことで非線形な分類ができるようになる。\n",
    "\n",
    "$$\n",
    "y = g(x),\\,  z = g(y)\n",
    "$$\n",
    "<img src='MLP.png' width='20%'>\n",
    "\n",
    "ニューラルネットワーク(NN)はより一般的な計算モデルで、MLPもその1つ。深層学習はDeep Neural Networkで複数の中間層を意味する。\n",
    "\n",
    "[様々なNN](https://www.asimovinstitute.org/wp-content/uploads/2019/04/NeuralNetworkZo19High.png)\n",
    "\n",
    "## 誤差逆伝播法（バックプロパゲーション）\n",
    "重みパラメータ$w$とバイアス$b$を調整して、入力から求めた予測値と正解データとの誤差が小さくするよう最適化して$(w,b)$を決定する。\n",
    "よいモデルであれば、どんな入力に対しても正解データに近い予測が得られるはず。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title {display-mode: \"form\"}\n",
    "# 活性化関数\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def step(z):\n",
    "    return np.heaviside(z, 0.5)\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y1 = sigmoid(x)\n",
    "y2 = step(x)\n",
    "y3 = ReLU(x)\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.plot(x, y1, label='sigmoid')\n",
    "plt.plot(x, y2, label='step')\n",
    "plt.plot(x, y3, label='ReLU')\n",
    "plt.legend()\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "#plt.savefig('activation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アヤメデータの読み込み\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.data[:2])\n",
    "print(iris.data.shape)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# データの正規化(平均0、分散1)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(iris.data)\n",
    "#x = iris.data\n",
    "x = scaler.transform(iris.data)\n",
    "print(x[:2])\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot 表現　0 -> [1,0,0], 1 -> [0,1,0], 2 -> [0,0,1] \n",
    "t = to_categorical(iris.target) # カテゴリー変数をone-hot表現に変換\n",
    "print(t[[49, 99, 149]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"split.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用と検証用に分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, t, test_size=0.3, random_state=0) \n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, t, test_size=0.3, random_state=0, stratify=t) # データセットの比率に合わせて分割（層化サンプリング）\n",
    "#stratify=t で各カテゴリの比率が同程度になるように分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "* 入力　4次元ベクトル（アヤメの特徴量）\n",
    "* 出力　3次元ベクトル（アヤメの種類の確率）\n",
    "* 中間層　M次元ベクトル（N層）M,Nの値は任意（ハイパーパラメータ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するライブラリ\n",
    "* scikit-learn 訓練用と検証用の分割、前処理などに使う\n",
    "* tensorflow 直接表には出てこないが、多次元配列の計算がGPUで高速化される\n",
    "* keras ニューラルネットワークを扱う\n",
    "* (pytorch というライブラリもあるが、ここでは扱わない)\n",
    "\n",
    "`model = Sequential([])`\n",
    "\n",
    "の中に層の種類と大きさ、活性化関数の種類を指定する。\n",
    "* `Input()` 入力層\n",
    "* `Dense()` 全結合層\n",
    "\n",
    "2次元画像に有用なCNNなどの発展形では更に増える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差関数 loss の取り方\n",
    "\n",
    "* 回帰問題も扱えるがここでは略\n",
    "* 2値分類　`BinaryCrossentropy()` 1(True), 0(False) \n",
    "* 多クラス分類 (one-hot エンコーディング) `CategoricalCrossentropy()` 1=[1,0,0],2=[0,1,0],3=[0,0,1]\n",
    "* 多クラス分類（整数ラベル） `SparseCategoricalCrossentropy()` 1,2,3\n",
    "\n",
    "この例ではアヤメのクラスが3種類あり、one-hotエンコーディングしているので`CategoricalCrossentropy()`を使う。\n",
    "\n",
    "犬か猫かという2値分類では`BinaryCrossentropy()`を使う。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='dnn.png' width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この例では中間層はN=2層, どちらも大きさはM=6\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "model = Sequential([\n",
    "    Input(shape=(4,)), # 入力層　（x_trainの形状に対応させる）\n",
    "    Dense(6, activation='relu'), # 中間層\n",
    "    Dense(6, activation='relu'), # 中間層\n",
    "    Dense(3, activation='softmax') # 出力層 (y_trainの形状)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練 epochs, batch_size はハイパーパラメータ\n",
    "# 1epochあたり 全サンプル数/バッチサイズ の回数だけ重みが更新される\n",
    "#tf.random.set_seed(0)\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=32, validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の様子を確認する　loss:誤差できるだけ小さく、accuracy:正解率できるだけ大きくなるのがベター\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練後の評価\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値\n",
    "y_pred = model.predict(x_test)\n",
    "print(y_pred[:10])\n",
    "print(y_test[:10])\n",
    "maxindex = np.argmax(y_pred, axis=1) # axis=0は行方向、axis=1は列方向で最大値のインデックス\n",
    "print(maxindex[:20])\n",
    "t_test = np.argmax(y_test, axis=1)\n",
    "print(t_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(t_test, maxindex)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の可視化\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "#plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "#plt.title('Confusion Matrix Heatmap (Multi-class)')\n",
    "plt.xticks(ticks=[0.5, 1.5, 2.5], labels=['Setosa', 'Versicolor', 'Virginica'])\n",
    "plt.yticks(ticks=[0.5, 1.5, 2.5], labels=['Setosa', 'Versicolor', 'Virginica'], rotation=0)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前回の練習問題にあったpenguinデータセットで同様の分類ができる。\n",
    "# サイズは大きいが乳がんデータ(sklear.datasets.load_breast_cancer)は2値分類であるので、損失関数はCategoricalCrossentropy からBinaryCrossentropyに、softmaxをsigmoidに変えればよい"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
